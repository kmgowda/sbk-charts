#!/usr/local/bin/python3
# Copyright (c) KMG. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
##
import  os
from statistics import mean
from huggingface_hub import InferenceClient
from src.genai.genai import SbkGenAI


HF_MODEL_ID = "google/gemma-2-2b-it"


class HuggingFace(SbkGenAI):
    def __init__(self):
        super().__init__()

    def _call_llm_for_throughput_analysis(self, prompt):
        """
        Use Hugging Face Inference API to generate the analysis text.
        """
        api_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not api_token:
            return [False, (
                "LLM analysis is not available (missing HUGGINGFACEHUB_API_TOKEN). "
                "Configure the token to enable Hugging Face-based analysis."
            )]

        client = InferenceClient(model=HF_MODEL_ID, token=api_token)

        completion = client.chat_completion(  # ← key change
            messages=[{"role": "user", "content": prompt}],
            max_tokens=400,
            temperature=0.4,
            top_p=0.9,
        )
        # OpenAI‑style response schema
        return [True, completion.choices[0].message["content"].strip()]

    
    def get_throughput_mb_analysis(self, throughputs, prompt_text = None):
        """
        Compare throughput values (MB/s) of multiple storage systems and return
        a descriptive analysis generated by a local LLM.

        throughput: {storage_name: [throughput_MBps, ...]}
        prompt_text: optional custom instruction; if None, a default prompt is used.
        """
        if not throughputs:
            return [False, "No throughput data provided; cannot perform analysis."]

        # 1) Compute per-system stats
        stats = {}
        for system, values in throughputs.items():
            clean_values = [v for v in values if v is not None]
            if not clean_values:
                continue
            stats[system] = {
                "min": min(clean_values),
                "max": max(clean_values),
                "avg": mean(clean_values),
                "count": len(clean_values),
            }

        if not stats:
            return [False, "No valid throughput values available for analysis."]

        # 2) Rank by average throughput (descending)
        ranked = sorted(stats.items(), key=lambda kv: kv[1]["avg"], reverse=True)

        # 3) Build a compact metrics table for the prompt
        lines = []
        for rank, (system, s) in enumerate(ranked, start=1):
            lines.append(
                f"{rank}. {system}: avg={s['avg']:.2f} MB/s, "
                f"min={s['min']:.2f}, max={s['max']:.2f}, n={s['count']}"
            )
        metrics_block = "\n".join(lines)

        """
        # for raw data
        lines =[]
        for i, (storage, values) in enumerate(throughputs.items()):
            lines.append(f"{i}. {storage}: {values}")
        metrics_block = "\n".join(lines)
        print(metrics_block)
        """

        # 4) Prompt engineering for a local instruct model
        default_instruction = (
            "You are a storage performance expert. "
            "Analyze the following throughput benchmark results for different storage systems. "
            "Throughput numbers are in MB/s, and higher values are better.\n\n"
            "Tasks:\n"
            "- Identify which storage systems have the highest and lowest average throughput.\n"
            "- Quantify relative differences roughly (for example, 'about 3x higher').\n"
            "- Mention any big gaps or interesting patterns.\n"
            "- Keep the analysis to 3–5 paragraphs.\n"
            "- if any sentence of paragraph is of more than 70 characters, break it into multiple sentences.\n"
            "Here are the measurements:\n"
            f"{metrics_block}\n\n"
            "Now write the analysis in clear, technical English."
        )

        if prompt_text:
            prompt = (
                    prompt_text.rstrip()
                    + "\n\nHere are the throughput metrics (MB/s):\n"
                    + metrics_block
                    + "\n\nNow generate the analysis."
            )
        else:
            prompt = default_instruction

        return self._call_llm_for_throughput_analysis(prompt)

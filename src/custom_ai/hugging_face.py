#!/usr/local/bin/python3
# Copyright (c) KMG. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
##
import  os
from statistics import mean
from huggingface_hub import InferenceClient
from src.genai.genai import SbkGenAI
from src.charts import constants

HF_MODEL_ID = "google/gemma-2-2b-it"


class HuggingFace(SbkGenAI):
    def __init__(self):
        super().__init__()

    def _call_llm_for_analysis(self, prompt):
        """
        Use Hugging Face Inference API to generate the analysis text.
        """
        api_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not api_token:
            return [False, (
                "LLM analysis is not available (missing HUGGINGFACEHUB_API_TOKEN). "
                "Configure the token to enable Hugging Face-based analysis."
            )]

        client = InferenceClient(model=HF_MODEL_ID, token=api_token)

        completion = client.chat_completion(  # ← key change
            messages=[{"role": "user", "content": prompt}],
            max_tokens=400,
            temperature=0.4,
            top_p=0.9,
        )
        # OpenAI‑style response schema
        return [True, completion.choices[0].message["content"].strip()]


    def get_model_description(self):
        return "Hugging Face Inference APIs with model ID: " + HF_MODEL_ID


    def get_throughput_analysis(self):
        """
        Compare throughput values (MB/s) of multiple storage systems and return
        a descriptive analysis generated by a local LLM.
        """

        # 1) Compute per-system stats
        stats = list()
        for stat in self.storage_stats:
            stats.append( {
                "storage": stat.storage,
                "min": min(stat.regular[constants.MB_PER_SEC]),
                "max": max(stat.regular[constants.MB_PER_SEC]),
                "avg": mean(stat.regular[constants.MB_PER_SEC]),
                "count": len(stat.regular[constants.MB_PER_SEC]),
            }
            )

        if not stats:
            return [False, "No valid throughput values available for analysis."]

        # 2) Rank by average throughput (descending)
        ranked = sorted(stats, key=lambda kv: kv["avg"], reverse=True)

        # 3) Build a compact metrics table for the prompt
        lines = []
        for rank, s in enumerate(ranked, start=1):
            lines.append(
                f"{rank}. {s['storage']}: avg={s['avg']:.2f} MB/s, "
                f"min={s['min']:.2f}, max={s['max']:.2f}, n={s['count']}"
            )
        metrics_block = "\n".join(lines)

        # 4) Prompt engineering for a local instruct model
        prompt = (
            "You are a storage performance expert. "
            "I need a detailed technical analysis of storage system latencies based on the following metrics. "
            "Analyze the following throughput benchmark results for different storage systems. "
            "Throughput numbers are in MB/s, and higher values are better.\n\n"
            "Tasks:\n"
            "- Identify which storage systems have the highest and lowest average throughput.\n"
            "- Quantify relative differences roughly (for example, 'about 3x higher').\n"
            "- Mention any big gaps or interesting patterns.\n"
            "- if any sentence of paragraph is of more than 70 characters, break it into multiple sentences.\n"
            "Here are the measurements:\n"
            f"{metrics_block}\n\n"
            "Now write the analysis in clear, technical English."
        )

        return self._call_llm_for_analysis(prompt)


    def get_latency_analysis(self):
        """
        Analyze latency metrics across different storage systems and return
        a descriptive analysis generated by a local LLM.
        
        Uses percentiles and other latency metrics from self.storage_stats
        to generate a comprehensive comparison and analysis.
        """
        if not self.storage_stats:
            return [False, "No storage statistics available for latency analysis."]

        # Define the key latency metrics we want to analyze
        latency_metrics = [
            constants.AVG_LATENCY,
            constants.MIN_LATENCY,
            constants.MAX_LATENCY,
            constants.PERCENTILE_50,  # Median
            constants.PERCENTILE_90,
            constants.PERCENTILE_95,
            constants.PERCENTILE_99,
            constants.PERCENTILE_99_9,
            constants.PERCENTILE_99_99
        ]

        # Collect latency statistics for each storage system
        stats = []
        for stat in self.storage_stats:
            if not stat.total:
                continue
                
            storage_stats = {
                constants.STORAGE: stat.storage,
                "action": stat.action,
                "metrics": {},
                "total_records": sum(stat.total.get(constants.RECORDS, [0]))
            }
            
            # Get values for each latency metric
            for metric in latency_metrics:
                if metric in stat.total:
                    values = stat.total[metric]
                    if values:
                        storage_stats["metrics"][metric] = {
                            "min": min(values),
                            "max": max(values),
                            "avg": sum(values) / len(values)
                        }
            
            # Add time unit for context
            storage_stats[constants.LATENCY_TIME_UNIT] = stat.timeunit
            stats.append(storage_stats)

        if not stats:
            return [False, "No valid latency data available for analysis."]

        # Build a comparison table for the prompt
        def format_value(metric_data, key):
            return f"{metric_data[key]:.2f}" if metric_data else "N/A"

        # Create a table with the most important metrics
        table_rows = []
        for stat in stats:
            metrics = stat["metrics"]
            row = [
                f"{stat[constants.STORAGE]} ({stat['action']})",
                format_value(metrics.get(constants.AVG_LATENCY, {}), "avg"),
                format_value(metrics.get(constants.PERCENTILE_50, {}), "avg"),
                format_value(metrics.get(constants.PERCENTILE_95, {}), "avg"),
                format_value(metrics.get(constants.PERCENTILE_99_9, {}), "avg"),
                format_value(metrics.get(constants.MAX_LATENCY, {}), "max"),
                f"{stat['total_records']:,}",
                stat[constants.LATENCY_TIME_UNIT]
            ]
            table_rows.append(" | ".join(row))

        # Create the prompt for the LLM
        prompt = (
            "I need a detailed technical analysis of storage system latencies based on the following metrics. "
            "The data represents various latency measurements across different storage systems.\n\n"
            "### Latency Metrics (all values in their respective time units):\n"
            "| Storage (Action) | Avg Latency | p50 (Median) | p95 | p99.9 | Max Latency | Total Records | Time Unit |\n"
            "|------------------|-------------|--------------|-----|-------|-------------|----------------|-----------|\n"
            f"{chr(10).join(table_rows)}\n\n"
            "### Analysis Instructions:\n"
            "1. Compare the latency profiles across different storage systems\n"
            "2. Identify which storage system performs best for different percentiles\n"
            "3. Note any significant differences between average and tail latencies\n"
            "4. Highlight any anomalies or interesting patterns in the data\n"
            "5. Consider the impact of the time unit (microseconds, milliseconds, etc.) and total records on the interpretation\n\n"
            "if any sentence of paragraph is of more than 70 characters, break it into multiple sentences.\n"
            "Provide a clear, concise technical analysis that would be useful for a storage engineer "
            "evaluating these systems. Focus on the most significant findings and their implications."
        )

        return self._call_llm_for_analysis(prompt)


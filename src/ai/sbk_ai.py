#!/usr/local/bin/python3
# Copyright (c) KMG. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
##
"""
SBK AI Analysis Module

This module provides AI-powered analysis of storage benchmark results through
the SbkAI class. It enables automated generation of performance insights,
including throughput and latency analysis, by leveraging various AI backends.

Supported AI Backends:
- Hugging Face (default)
- LM Studio (local models)
- Ollama (local LLMs)
- No-AI (stub implementation)

Key Features:
- Automated performance analysis of benchmark results
- Support for multiple AI providers
- Threaded execution for parallel analysis
- Configurable timeouts and error handling
"""
import time
from typing import final

from openpyxl import load_workbook

from src.ai.discover import discover_custom_ai_classes
from src.charts import constants
from src.charts.utils import is_r_num_sheet, get_columns_from_worksheet, get_storage_name_from_worksheet
from src.charts.utils import get_time_unit_from_worksheet, get_action_name_from_worksheet
from src.sheets import constants as sheets_constants
from openpyxl.styles import Font, Border, Side, Alignment   
import textwrap

from src.stat.storage import StorageStat
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
from src.rag.sbk_simple_rag import SbkSimpleRAGPipeline

# Log the full exception for debugging
import traceback

# Excel formatting constants
class ExcelColors:
    RED_BOLD = "FFFF0000"
    GREEN = "00CF00"
    PURPLE = "EE00FF"
    DARK_RED = "FF800000"
    GREEN_HEADER = "00AA00"
    DARK_BLUE = "FF000080"
    BLUE = "0000FF"
    DARK_GREEN = "008000"
    DARK_MAGENTA = "8B008B"
    SADDLE_BROWN = "8B4513"
    TITLE_RED = "FF0000"

# Warning message about AI-generated content reliability
warning_msg = ("The AI may hallucinate !. "
              "The summary generated by generative AI models may not be complete and accurate. "
              "It's recommended to analyze the graphs along with the generated summary.")

DEFAULT_TIMEOUT_SECONDS = 120

def get_t_num_sheet_name(r_num_name):
    """
    Return the corresponding T-sheet name for an R-sheet name.

    Example: 'R1' -> 'T1'

    Parameters:
        r_num_name (str): R-sheet name

    Returns:
        str: corresponding T-sheet name
    """
    return sheets_constants.T_PREFIX + r_num_name[1:]


def get_columns_values(ws):
    """
    Extract column values from a worksheet, excluding metadata columns.

    This function processes the data in a worksheet to extract all relevant
    performance metrics while excluding metadata columns like ID, HEADER, TYPE,
    STORAGE, ACTION, and LATENCY_TIME_UNIT.

    Args:
        ws (openpyxl.worksheet.worksheet.Worksheet): The worksheet to extract data from.

    Returns:
        dict: Dictionary mapping column names to their values as lists
    """
    columns = get_columns_from_worksheet(ws)
    ret = dict()
    for col_name, col_idx in columns.items():
        if col_name not in [constants.ID, constants.HEADER, constants.TYPE,
                          constants.STORAGE, constants.ACTION, constants.LATENCY_TIME_UNIT]:
            values = []
            for row in range(2, ws.max_row + 1):
                cell_value = ws.cell(row=row, column=col_idx).value
                values.append(cell_value)
            ret[col_name] = values
    return ret


@final
class SbkAI:
    """
    SBK AI Analysis Engine
    
    This class provides AI-powered analysis of storage benchmark results.
    It supports multiple AI backends and provides methods for generating
    performance insights, including throughput and latency analysis.
    
    The engine handles the complete workflow from loading benchmark data,
    running AI analyses in parallel, formatting results into Excel sheets,
    and saving the enhanced workbook with performance insights.

    Attributes:
        classes (dict): Dictionary of available AI backend classes discovered at runtime
        ai_instance_map (dict): Mapping of AI instances by name for easy access
        subparsers: Command-line argument subparsers for configuring different AI backends
        file (str): Path to the output Excel file being processed
        ai_instance: Active AI backend instance currently in use
        web: Web interface component (if enabled)
        timeout_seconds (int): Timeout for AI operations in seconds, default 120
        no_threads (bool): Flag to disable threaded execution for debugging purposes, default False
    """

    def __init__(self):
        """
        Initialize the SbkAI analysis engine.

        This constructor discovers all available AI backend classes and sets up
        the basic configuration for analysis execution.

        The discovery process uses reflection to find all classes that implement
        the AI interface in the custom_ai module.
        """
        self.classes = discover_custom_ai_classes()
        self.ai_instance_map = dict()
        self.subparsers = None
        self.input_files = None
        self.file =  None
        self.ai_instance = None
        self.web = None
        self.timeout_seconds = DEFAULT_TIMEOUT_SECONDS
        self.no_threads = False
        self.rag_pipeline = None

    def add_args(self, parser):
        """
        Add command-line arguments for AI configuration to the argument parser.

        This method configures the command-line interface with options specific
        to AI analysis, including timeout settings and threading controls.

        Args:
            parser (argparse.ArgumentParser): The argument parser to add arguments to

        Side Effects:
            - Adds timeout and threading configuration options
            - Registers subparsers for each available AI backend class
        """
        parser.add_argument("-secs", "--seconds", help=f"Timeout seconds, default : {self.timeout_seconds}",
                            default=self.timeout_seconds)
        parser.add_argument("-nothreads", "--nothreads", help=f"No parallel threads, default : {self.no_threads}",
                            default=self.no_threads)
        self.subparsers = parser.add_subparsers(dest="ai_class", help="Available GenAI commands", required=False)
        parser.set_defaults(ai_class=None)
        for name, cls in self.classes.items():
            try:
                subp = self.subparsers.add_parser(name)
                self.ai_instance_map[name.lower()] = cls()
                self.ai_instance_map[name.lower()].add_args(subp)
            except Exception:
                # don't break arg registration if a class has issues; log for debugging
                traceback.print_exc()


    def parse_args(self, args):
        """
        Parse command-line arguments and configure the AI instance.

        This method processes the parsed command-line arguments to set up
        the analysis engine with appropriate configuration and activate
        the selected AI backend.

        Args:
            args (argparse.Namespace): Parsed command-line arguments
            
        Side Effects:
            - Sets the output file path
            - Configures timeout (converted to int) and threading settings
            - Activates the selected AI backend instance if specified
            - Initializes RAG pipeline with input CSV files if available
        """
        self.timeout_seconds = int(args.seconds) if hasattr(args, 'seconds') and args.seconds is not None else DEFAULT_TIMEOUT_SECONDS
        self.file = args.ofile
        self.no_threads = args.nothreads
        self.input_files = args.ifiles.split(",")

        if args.ai_class:
            self.ai_instance = self.ai_instance_map[args.ai_class.lower()]
            self.ai_instance.parse_args(args)

    def _initialize_rag_pipeline(self):
        """
        Initialize the RAG pipeline with input CSV files.
        
        This method creates a RAG pipeline instance and ingests data from
        the input CSV files to provide context for AI analysis.
        Uses Simple RAG by default for maximum compatibility.
        """
        try:
            # Filter for CSV files from input files
            csv_files = [f for f in self.input_files if f.lower().endswith('.csv')]
            
            if not csv_files:
                print("No CSV files found in input files. RAG pipeline will not be initialized.")
                return
            
            print(f"Initializing RAG pipeline with CSV files: {csv_files}")
            
            # Use Simple RAG by default (maximum compatibility)
            print("üéØ Using Simple RAG (ChromaDB-free) for maximum compatibility...")
            try:
                self.rag_pipeline = SbkSimpleRAGPipeline()
                
                if self.rag_pipeline.initialize():
                    # Ingest CSV data
                    if self.rag_pipeline.ingest_csv_files(csv_files):
                        stats = self.rag_pipeline.get_collection_stats()
                        print(f"‚úÖ Simple RAG pipeline initialized successfully with {stats.get('document_count', 0)} documents")
                        print("üí° Simple RAG provides full functionality without external dependencies")
                        return
                    else:
                        print("‚ùå Failed to ingest CSV files into Simple RAG pipeline")
                else:
                    print("‚ùå Failed to initialize Simple RAG pipeline")
                    
            except Exception as e:
                print(f"‚ùå Simple RAG pipeline failed: {str(e)}")
            
            # Optional: Try ChromaDB as enhancement (not required)
            print("\nüîç Optional: Attempting ChromaDB for enhanced features...")
            try:
                self.rag_pipeline = SbkRAGPipeline()
                
                if self.rag_pipeline.initialize():
                    # Ingest CSV data
                    if self.rag_pipeline.ingest_csv_files(csv_files):
                        stats = self.rag_pipeline.get_collection_stats()
                        print(f"üéâ ChromaDB RAG pipeline initialized successfully with {stats.get('document_count', 0)} documents")
                        print("üöÄ Enhanced vector search features available")
                        return
                    else:
                        print("‚ö†Ô∏è Failed to ingest CSV files into ChromaDB RAG pipeline")
                else:
                    print("‚ö†Ô∏è Failed to initialize ChromaDB RAG pipeline")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è ChromaDB RAG pipeline failed: {str(e)}")
                print("   This is normal on some systems (especially Apple Silicon)")
            
            # If both failed, disable RAG
            print("\n‚ùå Both RAG pipelines failed. RAG functionality will be disabled.")
            self.rag_pipeline = None
                
        except Exception as e:
            print(f"‚ùå Error initializing RAG pipeline: {str(e)}")
            self.rag_pipeline = None

    def open(self, args):
        self._initialize_rag_pipeline()
        if self.ai_instance:
            self.ai_instance.open(args)


    def close(self, args):
        if self.ai_instance:
            self.ai_instance.close(args)
        
        # Clean up RAG pipeline if it exists
        if self.rag_pipeline:
            print("Closing RAG pipeline...")
            if self.rag_pipeline.close(cleanup_local_data=True):
                print("RAG pipeline closed successfully")
            else:
                print("Warning: Failed to close RAG pipeline properly")

    def load_workbook(self):
        """
        Load the Excel workbook from the specified file path.

        This method reads an existing Excel file and loads it into memory
        for further processing. It's typically called before any analysis
        or modification operations.

        Side Effects:
            - Loads the workbook into self.wb attribute
            - Raises IOError if file cannot be read

        Note:
            The workbook must exist and be in a valid Excel format.
        """
        self.wb = load_workbook(self.file)

    def save_workbook(self):
        """
        Save the current workbook to disk.

        This method writes any modifications made to the workbook back
        to the original file location. It should be called after all
        analysis and formatting operations are complete.

        Side Effects:
            - Writes the workbook to disk
            - Overwrites the original file

        Note:
            This operation will overwrite any existing data in the output file.
        """
        self.wb.save(self.file)

    def get_storage_stats(self):
        """
        Collect and organize storage statistics from all worksheets.
        
        This method processes each R-sheet in the workbook to extract
        performance metrics and organizes them into StorageStat objects.
        It handles both regular (R) and total (T) data sheets to provide
        comprehensive performance analysis.

        Returns:
            list: List of StorageStat objects containing performance metrics
                  for each storage system in the benchmark. Each object contains:
                  - Storage name
                  - Time unit
                  - Action type
                  - Regular performance metrics (from R sheets)
                  - Total performance metrics (from T sheets)

        Note:
            Only worksheets that match the R<number> pattern are processed.
        """
        stats = list()
        for name in self.wb.sheetnames:
            if is_r_num_sheet(name):
                ws = self.wb[name]
                storage = get_storage_name_from_worksheet(ws)
                timeunit = get_time_unit_from_worksheet(ws)
                action = get_action_name_from_worksheet(ws)
                # Get all the columns of R<Count>
                regular = get_columns_values(ws)
                # Get all the columns of T<Count>
                t_name = get_t_num_sheet_name(name)
                ws = self.wb[t_name]
                total = get_columns_values(ws)
                stats.append(StorageStat(storage, timeunit, action, regular, total))
        return stats

    def add_ai_analysis(self):
        """
        Create a summary sheet with AI-generated performance analysis.
        
        This method orchestrates the execution of all available AI analysis
        methods in parallel to improve performance. It collects throughput,
        latency, total MB, and percentile histogram analysis from the active
        AI backend and formats the results into Excel cells with appropriate styling.

        The method handles both sequential and parallel execution modes based
        on the no_threads configuration flag, with configurable timeouts to
        prevent hanging operations.
        
        Returns:
            bool: True if analysis was successful, False otherwise

        Side Effects:
            - Modifies the workbook by adding AI analysis content to Summary sheet
            - Sets various cell formatting including fonts, borders, and alignment
            - Adjusts row heights based on content length

        Note:
            This method requires the workbook to be loaded and an AI instance
            to be configured before calling.
        """
        # Set storage statistics for AI analysis
        self.ai_instance.set_storage_stats(self.get_storage_stats())
        
        # Set RAG pipeline if available
        if self.rag_pipeline:
            self.ai_instance.set_rag_pipeline(self.rag_pipeline)

        def run_analysis(function_name):
            """
            Execute a single AI analysis method and handle exceptions.

            This helper function wraps the execution of individual AI methods
            to provide consistent error handling and return format.

            Args:
                function_name (str): Name of the method to execute

            Returns:
                tuple: (method_name, result_tuple) where result_tuple is
                       (success_bool, analysis_content_or_error_message)
            """
            try:
                method = getattr(self.ai_instance, function_name)
                ret = method()
                return function_name,ret
            except Exception as e:
                print(f"Error in {function_name}: {str(e)}")
                return function_name, (False, str(e))
        
        # List of AI analysis methods to run in parallel
        analysis_methods = [
            'get_throughput_analysis',
            'get_latency_analysis',
            'get_total_mb_analysis',
            'get_percentile_histogram_analysis',
        ]

        # Run all analysis methods in parallel
        print("Starting AI analysis. This may take a few minutes...", flush=True)
        start_time = time.time()

        # Create a dictionary to store results
        results = {}

        if self.no_threads:
            # Run analysis methods sequentially for debugging or simpler execution
            print("Running analysis sequentially (no threads)...")
            for method_name in analysis_methods:
                try:
                    print(f"Running {method_name}...")
                    result_method, result = run_analysis(method_name)
                    results[result_method] = result
                    print(f"‚úì Completed {result_method}")

                    # Check timeout after each method
                    if (time.time() - start_time) > self.timeout_seconds:
                        print(f"‚ö†Ô∏è Timeout after {self.timeout_seconds} seconds")
                        # Mark remaining methods as timed out
                        for remaining in analysis_methods[analysis_methods.index(method_name) + 1:]:
                            results[remaining] = (False, "Analysis timed out")
                        break

                except Exception as e:
                    print(f"‚ö†Ô∏è Error in {method_name}: {str(e)}")
                    results[method_name] = (False, str(e))
        else:
            # Run analysis methods in parallel using ThreadPoolExecutor for better performance
            print(f"Running analysis in parallel with timeout: {self.timeout_seconds} seconds...")
            with ThreadPoolExecutor(max_workers=len(analysis_methods)) as executor:
                # Submit all tasks and store futures
                future_to_method = {executor.submit(run_analysis, method): method
                                    for method in analysis_methods}

                # Process completed tasks as they finish
                while future_to_method and (time.time() - start_time) < self.timeout_seconds:
                    # Wait for the next future to complete, with a timeout
                    done, _ = concurrent.futures.wait(
                        future_to_method.keys(),
                        timeout=2.0,  # Check every 2 seconds for timeouts
                        return_when=concurrent.futures.FIRST_COMPLETED
                    )

                    # Process completed futures
                    for future in done:
                        method_name = future_to_method.pop(future)
                        try:
                            result_method, result = future.result(timeout=1.0)
                            results[result_method] = result
                            print(f"‚úì Completed {result_method}")
                        except concurrent.futures.TimeoutError:
                            print(f"‚ö†Ô∏è Timeout waiting for {method_name}, skipping...")
                            results[method_name] = (False, "Analysis timed out")
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error in {method_name}: {str(e)}")
                            results[method_name] = (False, str(e))

                    # Print progress
                    remaining = len(future_to_method)
                    if remaining > 0:
                        print(f"‚è≥ Waiting for {remaining} more analysis tasks...")

                remaining = len(future_to_method)
                if remaining > 0:
                    print(f"‚ö†Ô∏è {remaining} analysis tasks Timeout !")
                    # Cancel any remaining futures
                    for future in future_to_method:
                        future.cancel()
                        method_name = future_to_method[future]
                        results[method_name] = (False, "Analysis timed out or failed")

        # Ensure all methods have a result
        for method_name in analysis_methods:
            if method_name not in results:
                results[method_name] = (False, "Analysis not completed")

        print(f"Analysis completed in {time.time() - start_time:.2f} seconds", flush=True)

       # Format and add AI analysis to the worksheet
        try:
            sheet = self.wb["Summary"]

            # Configure column H width to accommodate text
            sheet.column_dimensions['H'].width = 120 * 0.90
            
            # Add AI warning section with proper formatting
            max_row = sheet.max_row + 3  # Add spacing before the warning
            
            # Format and add the warning message
            warn_cell = sheet.cell(row=max_row, column=8)
            warn_cell.value = warning_msg
            warn_cell.font = Font(size=16, bold=True, color=ExcelColors.RED_BOLD)
            warn_cell.alignment = Alignment(wrap_text=True, vertical='top')
            
            # Calculate optimal row height for the warning message
            warning_wrapped_lines = []
            for line in warning_msg.split('\n'):
                warning_wrapped_lines.extend(textwrap.wrap(line, width=120))
            
            # Set row height with minimum of 35 points
            warn_row_height = max(35, len(warning_wrapped_lines) * 35)
            sheet.row_dimensions[max_row].height = warn_row_height

            # Add AI Performance Analysis section header
            max_row = sheet.max_row + 2
            title_cell = sheet.cell(row=max_row, column=7)
            title_cell.value = "AI Performance Analysis"
            title_cell.font = Font(size=18, bold=True, color=ExcelColors.TITLE_RED)
            
            # Add model description
            dec_cell = sheet.cell(row=max_row, column=8)
            dec_cell.value = self.ai_instance.get_model_description()[1]
            dec_cell.font = Font(size=16, color=ExcelColors.GREEN)

            # Add Throughput Analysis section
            throughput_header_row = max_row + 2
            cell = sheet.cell(row=throughput_header_row, column=7)
            cell.value = "Throughput Analysis"
            cell.font = Font(size=16, bold=True, color=ExcelColors.PURPLE)
            
            # Add throughput analysis content with formatting
            cell = sheet.cell(row=throughput_header_row, column=8)
            throughput_analysis = results['get_throughput_analysis'][1]
            cell.value = throughput_analysis
            cell.font = Font(size=14, color=ExcelColors.DARK_RED)
            cell.border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            cell.alignment = Alignment(wrap_text=True, vertical='top')
            
            # Calculate and set optimal row height for throughput analysis
            wrapped_lines = []
            for line in throughput_analysis.split('\n'):
                wrapped_lines.extend(textwrap.wrap(line, width=120))
            
            row_height = max(25, len(wrapped_lines) * 25)
            sheet.row_dimensions[throughput_header_row].height = row_height

            # Add Latency Analysis section
            latency_row = sheet.max_row + 1
            
            # Format and add latency analysis header
            cell = sheet.cell(row=latency_row, column=7)
            cell.value = "Latency Analysis"
            cell.font = Font(size=16, bold=True, color=ExcelColors.GREEN_HEADER)
            
            # Add latency analysis content with formatting
            cell = sheet.cell(row=latency_row, column=8)
            latency_analysis = results['get_latency_analysis'][1]
            cell.value = latency_analysis
            cell.font = Font(size=14, color="FF000080")
            cell.border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            cell.alignment = Alignment(wrap_text=True, vertical='top')
            
            # Calculate and set optimal row height for latency analysis
            latency_wrapped_lines = []
            for line in latency_analysis.split('\n'):
                latency_wrapped_lines.extend(textwrap.wrap(line, width=120))
            
            latency_row_height = max(25, len(latency_wrapped_lines) * 25)
            sheet.row_dimensions[latency_row].height = latency_row_height
            
            # Add Total MB Analysis section
            mb_row = sheet.max_row + 1
            
            # Format and add total MB analysis header
            cell = sheet.cell(row=mb_row, column=7)
            cell.value = "Total MB Analysis"
            cell.font = Font(size=16, bold=True, color=ExcelColors.BLUE)
            
            # Add total MB analysis content with formatting
            cell = sheet.cell(row=mb_row, column=8)
            mb_analysis = results['get_total_mb_analysis'][1]
            cell.value = mb_analysis
            cell.font = Font(size=14, color=ExcelColors.DARK_GREEN)
            cell.border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            cell.alignment = Alignment(wrap_text=True, vertical='top')
            
            # Calculate and set optimal row height for total MB analysis
            mb_wrapped_lines = []
            for line in mb_analysis.split('\n'):
                mb_wrapped_lines.extend(textwrap.wrap(line, width=120))
            
            mb_row_height = max(25, len(mb_wrapped_lines) * 25)
            sheet.row_dimensions[mb_row].height = mb_row_height
            
            # Add Percentile Histogram Analysis section
            percentile_row = sheet.max_row + 1  # Add an extra blank row
            
            # Format and add percentile histogram analysis header
            cell = sheet.cell(row=percentile_row, column=7)
            cell.value = "Percentile Histogram Analysis"
            cell.font = Font(size=16, bold=True, color=ExcelColors.DARK_MAGENTA)
            
            # Add percentile histogram analysis content with formatting
            cell = sheet.cell(row=percentile_row, column=8)
            percentile_analysis = results['get_percentile_histogram_analysis'][1]
            cell.value = percentile_analysis
            cell.font = Font(size=14, color=ExcelColors.SADDLE_BROWN)
            cell.border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            cell.alignment = Alignment(wrap_text=True, vertical='top')
            
            # Calculate and set optimal row height for percentile histogram analysis
            percentile_wrapped_lines = []
            for line in percentile_analysis.split('\n'):
                percentile_wrapped_lines.extend(textwrap.wrap(line, width=120))
            
            percentile_row_height = max(25, len(percentile_wrapped_lines) * 25)
            sheet.row_dimensions[percentile_row].height = percentile_row_height

        except Exception as e:
            print(f"Error adding analysis to summary sheet: {str(e)}")
            traceback.print_exc()
        
        return True

    def add_performance_details(self):
        """
        Generate all performance graphs and AI analysis for the benchmark results.
        
        This is the main entry point method that orchestrates the complete workflow:
        1. Loads the Excel workbook
        2. Runs AI analysis on performance data
        3. Saves the enhanced workbook with graphs and AI documentation
        
        The method creates various performance visualizations including:
        - Throughput graphs (MB/s and records/s)
        - Latency comparison graphs
        - Write/Read metrics graphs
        - Percentile analysis graphs
        - Comparative analysis graphs
        
        After generating all visualizations and AI insights, it saves the final
        enhanced workbook to disk.

        Side Effects:
            - Loads and modifies an Excel workbook
            - Saves the modified workbook to disk
            - Prints status messages during execution

        Note:
            This method should only be called when an AI backend is properly configured.
            If no AI instance is set, it will display a warning message and skip analysis.
        """
        if not self.ai_instance:
            print("AI is not enabled!. you can use the subcommands ["+" ".join(self.classes.keys())+"] to enable it.")
        else:
            self.load_workbook()
            if self.add_ai_analysis():
                print(f"File updated with graphs and AI documentation: {self.file}")
            self.save_workbook()

